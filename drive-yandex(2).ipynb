{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11356834,"sourceType":"datasetVersion","datasetId":7107451}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read data example","metadata":{"id":"ULyIGnA4hVXi","editable":false}},{"cell_type":"code","source":"import os\nimport typing\nfrom tqdm import tqdm\nimport glob\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport json\n\nfrom keras.utils import  Sequence\n\n","metadata":{"id":"oDoV7ypBhVXq","executionInfo":{"status":"ok","timestamp":1743780895745,"user_tz":-180,"elapsed":3970,"user":{"displayName":"Kallen 14","userId":"05555523211482635477"}},"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-28T15:16:37.102586Z","iopub.execute_input":"2025-04-28T15:16:37.103039Z","iopub.status.idle":"2025-04-28T15:16:54.862670Z","shell.execute_reply.started":"2025-04-28T15:16:37.103008Z","shell.execute_reply":"2025-04-28T15:16:54.861798Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"ROOT_DATA_FOLDER = r\"/kaggle/input/drive-redused\"\n\nTRAIN_DATASET_PATH = os.path.join(ROOT_DATA_FOLDER, r\"YaCupTrain\")\n\nTEST_DATASET_PATH = os.path.join(ROOT_DATA_FOLDER, r\"YaCupTest\")\nlabel_columns = ['x', 'y', 'yaw']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:16:54.863968Z","iopub.execute_input":"2025-04-28T15:16:54.864595Z","iopub.status.idle":"2025-04-28T15:16:54.869893Z","shell.execute_reply.started":"2025-04-28T15:16:54.864552Z","shell.execute_reply":"2025-04-28T15:16:54.868598Z"},"editable":false},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Read Data","metadata":{"editable":false}},{"cell_type":"code","source":"\n# Load all ids of a dataset\n\ndef read_testcase_ids(dataset_path: str):\n    ids = [int(case_id) for case_id in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, case_id))]\n    return ids\n\nids = read_testcase_ids(TRAIN_DATASET_PATH)\nlen(ids)\n\ntrain_ids = np.random.choice(ids, size=round(0.70*len(ids)), replace=False, p=None)\ntest_ids = [el for el in ids if el not in train_ids]\n\n\nclass DataFilePaths:\n    def __init__(self, testcase_path: str):\n        self.testcase_path = testcase_path\n\n    def localization(self):\n        return os.path.join(self.testcase_path, 'localization.csv')\n\n    def control(self):\n        return os.path.join(self.testcase_path, 'control.csv')\n\n    def metadata(self):\n        return os.path.join(self.testcase_path, 'metadata.json')\n\n    # exists only for test_dataset\n    def requested_stamps(self):\n        return os.path.join(self.testcase_path, 'requested_stamps.csv')\n\n\ndef read_localization(localization_path: str):\n    return pd.read_csv(localization_path)\n\ndef read_control(control_path):\n    return pd.read_csv(control_path)\n\ndef read_metadata(metadata_path: str):\n    with open(metadata_path, 'r') as f:\n        data = json.load(f)\n    return data\n\ndef read_requested_stamps(requested_stamps_path: str):\n    return pd.read_csv(requested_stamps_path)\n\ndef read_testcase(dataset_path: str, testcase_id: str, is_test: bool = False):\n    testcase_path = os.path.join(dataset_path, str(testcase_id))\n    data_file_paths = DataFilePaths(testcase_path)\n\n    testcase_data = {}\n    testcase_data['localization'] = read_localization(data_file_paths.localization())\n    testcase_data['control'] = read_control(data_file_paths.control())\n    testcase_data['metadata'] = read_metadata(data_file_paths.metadata())\n    if is_test:\n        testcase_data['requested_stamps'] = read_requested_stamps(data_file_paths.requested_stamps())\n\n    return testcase_data\n\n\ndef read_testcases(dataset_path: str, is_test: bool = False, testcase_ids: typing.Iterable[int] = None):\n    result = {}\n    if testcase_ids is None:\n        testcase_ids = read_testcase_ids(dataset_path)\n\n    for testcase_id in tqdm(testcase_ids):\n        testcase = read_testcase(dataset_path, testcase_id, is_test=is_test)\n        result[testcase_id] = testcase\n    return result","metadata":{"id":"bCpSHxcNhVXv","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:16:54.871733Z","iopub.execute_input":"2025-04-28T15:16:54.872177Z","iopub.status.idle":"2025-04-28T15:17:00.890979Z","shell.execute_reply.started":"2025-04-28T15:16:54.872126Z","shell.execute_reply":"2025-04-28T15:17:00.889547Z"},"editable":false},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_dataset = read_testcases(TRAIN_DATASET_PATH)\nlen(train_dataset)","metadata":{"id":"Z7nU5sPYhVXw","outputId":"9ef4241a-bc40-484a-8f2f-cae65e2104f4","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:17:00.892572Z","iopub.execute_input":"2025-04-28T15:17:00.892994Z","iopub.status.idle":"2025-04-28T15:18:44.695747Z","shell.execute_reply.started":"2025-04-28T15:17:00.892954Z","shell.execute_reply":"2025-04-28T15:18:44.694641Z"},"editable":false},"outputs":[{"name":"stderr","text":"100%|██████████| 3884/3884 [01:43<00:00, 37.43it/s]\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"3884"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"\ntrain = {k: v for k, v in train_dataset.items() if k in train_ids}\ntest = {k: v for k, v in train_dataset.items() if k in test_ids}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:18:44.696981Z","iopub.execute_input":"2025-04-28T15:18:44.697287Z","iopub.status.idle":"2025-04-28T15:18:44.778025Z","shell.execute_reply.started":"2025-04-28T15:18:44.697260Z","shell.execute_reply":"2025-04-28T15:18:44.776805Z"},"editable":false},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Window Generate","metadata":{}},{"cell_type":"markdown","source":"## 1 dict to df + mixed bach size","metadata":{"editable":false}},{"cell_type":"code","source":"class WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train, val,\n               label_columns=None):\n    # Store the raw data.\n    self.train = train\n    self.val = test\n\n    train_df = self.dict_to_df(train)\n    val_df = self.dict_to_df(test)\n\n\n\n    \n    \n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n\n  def dict_to_df(self, dict_df):\n      for i in dict_df.keys():\n          return dict_df[i]['localization']\n      \n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-21T06:25:52.037379Z","iopub.execute_input":"2025-04-21T06:25:52.037725Z","iopub.status.idle":"2025-04-21T06:25:52.045581Z","shell.execute_reply.started":"2025-04-21T06:25:52.037690Z","shell.execute_reply":"2025-04-21T06:25:52.044638Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## 2 dict to df + mixed bach size logic","metadata":{"editable":false}},{"cell_type":"code","source":"class WindowGenerator(Sequence):\n  def __init__(self, input_width, label_width, shift,\n               train, val,\n               label_columns=None, batch_size=32):\n    # Store the raw data.\n    self.train = train\n    self.val = test\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n    self.batch_size =  batch_size\n    buch_for_buch = round(buch_data_len - label_width/shift) # quality of el from dict with we use for one buch\n\n\n\n  def __len__(self):\n    'Denotes the number of batches per epoch'\n    len_after_windows = train.keys()*buch_data_len/input_width*shift\n    return int(np.floor(len(len_after_windows) / self.batch_size))\n\n    \n  def on_epoch_end(self):\n    'Updates indexes after each epoch'\n    self.indexes = np.arange(len(self.)) #!!!!!\n    if self.shuffle == True:\n        np.random.shuffle(self.indexes)\n\n\n  def __getitem__(self, index):\n        'Generate one batch of data'\n    # Generate indexes of the batch\n    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n    # Find list of IDs\n    list_IDs_temp = [k for k in indexes]\n\n    # Generate data\n    X, y = self.__data_generation(list_IDs_temp)\n\n    return X, y\n\n\n   def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        batch_data = list() \n        batch_labels = list()\n        \n        # Generate data\n        for i in train.keys():\n            # Store sample\n\n            batch_data.append()\n            #Store class\n            batch_labels.append(label)\n            \n        return np.array(batch_data), np.array(batch_labels)\n      \n  def dict_to_df(self, dict_df):\n      for i in dict_df.keys():\n          return dict_df[i]['localization']\n\n\n  def split_window(self, features):\n    inputs = features[:, self.input_slice, :]\n    labels = features[:, self.labels_slice, :]\n    if self.label_columns is not None:\n      labels = tf.stack(\n          [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n           axis=-1)\n\n    # Slicing doesn't preserve static shape information, so set the shapes\n    # manually. This way the `tf.data.Datasets` are easier to inspect.\n    inputs.set_shape([None, self.input_width, None])\n    labels.set_shape([None, self.label_width, None])\n\n    return inputs, labels\n\n  def bach_prepare():\n      \n      \n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## With UP Class Logic","metadata":{"editable":false}},{"cell_type":"code","source":"class WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df, val_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])\n\n\n\n\nclass UPWindowGenerator():\n    def __init__(train_dict, val_dict,):\n        self.train_dict = train_dict\n        self.batch_size =  batch_size\n        #self.val_df = val_df\n\n    def get_items():\n        bach \n        for i in train_dict.keys():\n            \n            WindowGenerator(train_dict[i]['localization'])\n\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## step by step logic","metadata":{"editable":false}},{"cell_type":"code","source":"class WindowGenerator():\n  def __init__(self,train_df, input_width, label_width, shift,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:16:31.702286Z","iopub.execute_input":"2025-04-28T15:16:31.702617Z","iopub.status.idle":"2025-04-28T15:16:31.711613Z","shell.execute_reply.started":"2025-04-28T15:16:31.702589Z","shell.execute_reply":"2025-04-28T15:16:31.709785Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\n\nWindowGenerator.split_window = split_window","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:18:44.779806Z","iopub.execute_input":"2025-04-28T15:18:44.780195Z","iopub.status.idle":"2025-04-28T15:18:44.787044Z","shell.execute_reply.started":"2025-04-28T15:18:44.780150Z","shell.execute_reply":"2025-04-28T15:18:44.785898Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_seq_X = []\ntrain_seq_y = []\n\nfor key in train.keys():\n    w2 = WindowGenerator(train[key]['localization'], input_width=5, label_width=5, shift=1, label_columns=['x','y','yaw'])\n    example_window = tf.stack([np.array(train[key]['localization'][:w2.total_window_size]),\n                           np.array(train[key]['localization'][100:100+w2.total_window_size]),\n                           np.array(train[key]['localization'][200:200+w2.total_window_size])\n                           ])\n  \n    X , y = w2.split_window(example_window)\n\n    train_seq_X.append(X)\n    train_seq_y.append(y)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:18:44.788364Z","iopub.execute_input":"2025-04-28T15:18:44.788876Z","iopub.status.idle":"2025-04-28T15:18:51.037220Z","shell.execute_reply.started":"2025-04-28T15:18:44.788784Z","shell.execute_reply":"2025-04-28T15:18:51.036020Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"A = np.array(train_seq_X)\nA.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:49:09.655952Z","iopub.execute_input":"2025-04-18T13:49:09.656332Z","iopub.status.idle":"2025-04-18T13:49:09.677286Z","shell.execute_reply.started":"2025-04-18T13:49:09.656305Z","shell.execute_reply":"2025-04-18T13:49:09.676288Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"(2719, 3, 5, 7)"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"train_seq_X = []\ntrain_seq_y = []\n\nfor key in train.keys():\n    w2 = WindowGenerator(train[key]['localization'], input_width=5, label_width=5, shift=1, label_columns=['x','y','yaw'])\n    example_window = tf.stack([np.array(train[key]['localization'][:w2.total_window_size]),\n                           np.array(train[key]['localization'][100:100+w2.total_window_size]),\n                           np.array(train[key]['localization'][200:200+w2.total_window_size])\n                           ])\n  \n    X , y = w2.split_window(example_window)\n\n    for X_i, y_i in zip(X , y):\n\n         train_seq_X.append(X_i)\n         train_seq_y.append(y_i)\n    \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:46:06.886780Z","iopub.execute_input":"2025-04-28T15:46:06.887143Z","iopub.status.idle":"2025-04-28T15:46:17.560738Z","shell.execute_reply.started":"2025-04-28T15:46:06.887117Z","shell.execute_reply":"2025-04-28T15:46:17.559500Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"train_seq_X= np.array(train_seq_X)\ntrain_seq_y = np.array(train_seq_y)\n## Add normalize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:46:17.562132Z","iopub.execute_input":"2025-04-28T15:46:17.562512Z","iopub.status.idle":"2025-04-28T15:46:17.667002Z","shell.execute_reply.started":"2025-04-28T15:46:17.562480Z","shell.execute_reply":"2025-04-28T15:46:17.665823Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_seq_X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:19:05.034203Z","iopub.execute_input":"2025-04-28T15:19:05.034606Z","iopub.status.idle":"2025-04-28T15:19:05.043400Z","shell.execute_reply.started":"2025-04-28T15:19:05.034572Z","shell.execute_reply":"2025-04-28T15:19:05.041821Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(8157, 5, 7)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"max_m = max(train_seq_X[:,:,0].flatten())\nmin_m = min(train_seq_X[:,:,0].flatten())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:27:05.964079Z","iopub.execute_input":"2025-04-28T15:27:05.964462Z","iopub.status.idle":"2025-04-28T15:27:05.977315Z","shell.execute_reply.started":"2025-04-28T15:27:05.964430Z","shell.execute_reply":"2025-04-28T15:27:05.976146Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train_seq_X.shape[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:28:46.861990Z","iopub.execute_input":"2025-04-28T15:28:46.862305Z","iopub.status.idle":"2025-04-28T15:28:46.868317Z","shell.execute_reply.started":"2025-04-28T15:28:46.862279Z","shell.execute_reply":"2025-04-28T15:28:46.867114Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"7"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## custom handwrited minmax scaller","metadata":{}},{"cell_type":"code","source":"\n\nfor lindex in range(train_seq_X.shape[2]):\n    max_m = max(train_seq_X[:,:,lindex].flatten())\n    min_m = min(train_seq_X[:,:,lindex].flatten())\n    for findex in range(train_seq_X.shape[0]):\n        for el in range(train_seq_X.shape[1]):\n            train_seq_X[findex,el,lindex] =  (train_seq_X[findex,el,lindex] -min_m)/(max_m-min_m)\n        \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:46:25.942722Z","iopub.execute_input":"2025-04-28T15:46:25.943077Z","iopub.status.idle":"2025-04-28T15:46:26.258999Z","shell.execute_reply.started":"2025-04-28T15:46:25.943047Z","shell.execute_reply":"2025-04-28T15:46:26.257864Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"for lindex in range(train_seq_y.shape[2]):\n    max_m = max(train_seq_y[:,:,lindex].flatten())\n    min_m = min(train_seq_y[:,:,lindex].flatten())\n    for findex in range(train_seq_y.shape[0]):\n        for el in range(train_seq_y.shape[1]):\n            train_seq_y[findex,el,lindex] =  (train_seq_y[findex,el,lindex] -min_m)/(max_m-min_m)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:46:35.927915Z","iopub.execute_input":"2025-04-28T15:46:35.928256Z","iopub.status.idle":"2025-04-28T15:46:36.071786Z","shell.execute_reply.started":"2025-04-28T15:46:35.928229Z","shell.execute_reply":"2025-04-28T15:46:36.070676Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"max(train_seq_X.flatten())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:46:38.447040Z","iopub.execute_input":"2025-04-28T15:46:38.447530Z","iopub.status.idle":"2025-04-28T15:46:38.496740Z","shell.execute_reply.started":"2025-04-28T15:46:38.447487Z","shell.execute_reply":"2025-04-28T15:46:38.495769Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"1.0"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"min(train_seq_y.flatten())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:46:55.907651Z","iopub.execute_input":"2025-04-28T15:46:55.908056Z","iopub.status.idle":"2025-04-28T15:46:55.925156Z","shell.execute_reply.started":"2025-04-28T15:46:55.908022Z","shell.execute_reply":"2025-04-28T15:46:55.924219Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"0.0"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_seq_y.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T07:05:13.821781Z","iopub.execute_input":"2025-04-21T07:05:13.822142Z","iopub.status.idle":"2025-04-21T07:05:13.827547Z","shell.execute_reply.started":"2025-04-21T07:05:13.822108Z","shell.execute_reply":"2025-04-21T07:05:13.826527Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(8157, 5, 3)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"B = np.array(train_seq_X)\nB.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:26:01.910587Z","iopub.execute_input":"2025-04-21T06:26:01.910876Z","iopub.status.idle":"2025-04-21T06:26:01.957954Z","shell.execute_reply.started":"2025-04-21T06:26:01.910844Z","shell.execute_reply":"2025-04-21T06:26:01.956947Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(8157, 5, 7)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(512, return_sequences=True),\n    tf.keras.layers.LSTM(128, return_sequences=True),\n    tf.keras.layers.LSTM(3, return_sequences=True)\n    # Shape => [batch, time, features]\n    #tf.keras.layers.Dense(units=5,3)\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:47:20.206822Z","iopub.execute_input":"2025-04-28T15:47:20.207178Z","iopub.status.idle":"2025-04-28T15:47:20.339575Z","shell.execute_reply.started":"2025-04-28T15:47:20.207153Z","shell.execute_reply":"2025-04-28T15:47:20.338335Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:47:23.638605Z","iopub.execute_input":"2025-04-28T15:47:23.639041Z","iopub.status.idle":"2025-04-28T15:47:23.644390Z","shell.execute_reply.started":"2025-04-28T15:47:23.639009Z","shell.execute_reply":"2025-04-28T15:47:23.643039Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"lstm_model.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n    loss=keras.losses.SquaredHinge(),\n    metrics=[\n        keras.metrics.Accuracy(), keras.metrics.RootMeanSquaredError(), keras.metrics.CosineSimilarity()\n    ],\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:47:25.386765Z","iopub.execute_input":"2025-04-28T15:47:25.387114Z","iopub.status.idle":"2025-04-28T15:47:25.422308Z","shell.execute_reply.started":"2025-04-28T15:47:25.387089Z","shell.execute_reply":"2025-04-28T15:47:25.420837Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"losses.Tversky()\nSquaredHinge","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"lstm_model.fit(train_seq_X, train_seq_y, epochs=20, batch_size=2028)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:47:31.948230Z","iopub.execute_input":"2025-04-28T15:47:31.948641Z","iopub.status.idle":"2025-04-28T15:48:57.945355Z","shell.execute_reply.started":"2025-04-28T15:47:31.948606Z","shell.execute_reply":"2025-04-28T15:48:57.944262Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 740ms/step - accuracy: 0.0000e+00 - cosine_similarity: -0.7384 - loss: 1.0058 - root_mean_squared_error: 0.6374\nEpoch 2/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 795ms/step - accuracy: 0.0000e+00 - cosine_similarity: -0.6072 - loss: 1.0036 - root_mean_squared_error: 0.6364\nEpoch 3/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 712ms/step - accuracy: 0.0000e+00 - cosine_similarity: -0.3712 - loss: 1.0015 - root_mean_squared_error: 0.6343\nEpoch 4/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 689ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.0347 - loss: 0.9994 - root_mean_squared_error: 0.6326\nEpoch 5/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 679ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.4165 - loss: 0.9972 - root_mean_squared_error: 0.6310\nEpoch 6/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 699ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.6230 - loss: 0.9950 - root_mean_squared_error: 0.6303\nEpoch 7/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 683ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.7235 - loss: 0.9930 - root_mean_squared_error: 0.6279\nEpoch 8/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 683ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.7791 - loss: 0.9908 - root_mean_squared_error: 0.6268\nEpoch 9/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 687ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.8198 - loss: 0.9887 - root_mean_squared_error: 0.6235\nEpoch 10/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 675ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.8456 - loss: 0.9866 - root_mean_squared_error: 0.6225\nEpoch 11/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 786ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.8661 - loss: 0.9843 - root_mean_squared_error: 0.6219\nEpoch 12/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 676ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.8773 - loss: 0.9823 - root_mean_squared_error: 0.6193\nEpoch 13/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 676ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.8849 - loss: 0.9801 - root_mean_squared_error: 0.6172\nEpoch 14/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 678ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.8931 - loss: 0.9780 - root_mean_squared_error: 0.6144\nEpoch 15/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 684ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.9007 - loss: 0.9757 - root_mean_squared_error: 0.6139\nEpoch 16/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 685ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.9051 - loss: 0.9735 - root_mean_squared_error: 0.6121\nEpoch 17/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 690ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.9080 - loss: 0.9714 - root_mean_squared_error: 0.6107\nEpoch 18/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 688ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.9136 - loss: 0.9691 - root_mean_squared_error: 0.6088\nEpoch 19/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 783ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.9153 - loss: 0.9669 - root_mean_squared_error: 0.6072\nEpoch 20/20\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 682ms/step - accuracy: 0.0000e+00 - cosine_similarity: 0.9176 - loss: 0.9647 - root_mean_squared_error: 0.6045\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7cb8d609fb80>"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lstm_model.compile(train_seq_X, train_seq_y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:31:21.745906Z","iopub.execute_input":"2025-04-21T06:31:21.746249Z","iopub.status.idle":"2025-04-21T06:31:21.752154Z","shell.execute_reply.started":"2025-04-21T06:31:21.746223Z","shell.execute_reply":"2025-04-21T06:31:21.751125Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<Sequential name=sequential, built=False>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_seq_X = tf.zeros([5,7,1],dtype=tf.dtypes.float64,)\ntrain_seq_y = tf.zeros([5,3,1],dtype=tf.dtypes.float64,)\nprint(train_seq_X.ndim, train_seq_X.shape)\nfor key in train.keys():\n    w2 = WindowGenerator(train[key]['localization'], input_width=5, label_width=5, shift=1, label_columns=['x','y','yaw'])\n    example_window = tf.stack([np.array(train[key]['localization'][:w2.total_window_size]),\n                           np.array(train[key]['localization'][100:100+w2.total_window_size]),\n                           np.array(train[key]['localization'][200:200+w2.total_window_size])\n                           ])\n  \n    X , y = w2.split_window(example_window)\n    #print(X)\n    for X_i, y_i in zip(X , y):\n        #print(X_i.ndim, X_i.shape)\n        train_seq_X = tf.concat([train_seq_X,X_i], 1 )\n        train_seq_y =  tf.concat([train_seq_y,y_i], 1 )\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:40:17.463333Z","iopub.execute_input":"2025-04-18T13:40:17.463718Z","iopub.status.idle":"2025-04-18T13:40:17.493819Z","shell.execute_reply.started":"2025-04-18T13:40:17.463686Z","shell.execute_reply":"2025-04-18T13:40:17.492567Z"}},"outputs":[{"name":"stdout","text":"3 (5, 7, 1)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-0c4c1729a434>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#print(X_i.ndim, X_i.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_seq_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_seq_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_seq_y\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_seq_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5982\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5983\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [5,7,1] vs. shape[1] = [5,7] [Op:ConcatV2] name: concat"],"ename":"InvalidArgumentError","evalue":"{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [5,7,1] vs. shape[1] = [5,7] [Op:ConcatV2] name: concat","output_type":"error"}],"execution_count":47},{"cell_type":"code","source":"train_seq_X.shape()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T07:03:50.529502Z","iopub.execute_input":"2025-04-21T07:03:50.529832Z","iopub.status.idle":"2025-04-21T07:03:50.549568Z","shell.execute_reply.started":"2025-04-21T07:03:50.529808Z","shell.execute_reply":"2025-04-21T07:03:50.547856Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-0f2ea6501dd8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_seq_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"],"ename":"AttributeError","evalue":"'list' object has no attribute 'shape'","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"\nprint(train_seq_X.ndim, train_seq_X.shape)\nfor key in train.keys():\n    w2 = WindowGenerator(train[key]['localization'], input_width=5, label_width=5, shift=1, label_columns=['x','y','yaw'])\n    example_window = tf.stack([np.array(train[key]['localization'][:w2.total_window_size]),\n                           np.array(train[key]['localization'][100:100+w2.total_window_size]),\n                           np.array(train[key]['localization'][200:200+w2.total_window_size])\n                           ])\n  \n    X , y = w2.split_window(example_window)\n    #print(X)\n    train_seq_X = tf.zeros([5,7],dtype=tf.dtypes.float64,)\n    train_seq_y = tf.zeros([5,3],dtype=tf.dtypes.float64,)\n    for X_i, y_i in zip(X , y):\n        #print(X_i.ndim, X_i.shape)\n        train_seq_X = tf.concat([train_seq_X,X_i], 0 )\n        train_seq_y =  tf.concat([train_seq_y,y_i], 0 )\n\ntrain_seq_X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_seq_X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:35:33.693780Z","iopub.execute_input":"2025-04-18T13:35:33.694103Z","iopub.status.idle":"2025-04-18T13:35:33.699250Z","shell.execute_reply.started":"2025-04-18T13:35:33.694077Z","shell.execute_reply":"2025-04-18T13:35:33.698453Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(285495,)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_seq_y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:25:58.830064Z","iopub.execute_input":"2025-04-18T13:25:58.830442Z","iopub.status.idle":"2025-04-18T13:25:58.836715Z","shell.execute_reply.started":"2025-04-18T13:25:58.830401Z","shell.execute_reply":"2025-04-18T13:25:58.835801Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(40790, 3), dtype=float64, numpy=\narray([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       ...,\n       [ 2.20955557e+04, -6.52065807e+04,  2.06363593e+00],\n       [ 2.20954131e+04, -6.52063152e+04,  2.06305254e+00],\n       [ 2.20952699e+04, -6.52060482e+04,  2.06283024e+00]])>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"train_seq_X = np.array([])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:28:46.890593Z","iopub.execute_input":"2025-04-18T13:28:46.890946Z","iopub.status.idle":"2025-04-18T13:28:46.895608Z","shell.execute_reply.started":"2025-04-18T13:28:46.890918Z","shell.execute_reply":"2025-04-18T13:28:46.894310Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_seq_X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:28:48.551616Z","iopub.execute_input":"2025-04-18T13:28:48.551978Z","iopub.status.idle":"2025-04-18T13:28:48.557458Z","shell.execute_reply.started":"2025-04-18T13:28:48.551947Z","shell.execute_reply":"2025-04-18T13:28:48.556493Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"array([], dtype=float64)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"\ntrain_seq_X = np.array([[]])\n#print(train_seq_X.ndim, train_seq_X.shape)\n#train_seq_y = np.array([])\nfor key in train.keys():\n    w2 = WindowGenerator(train[key]['localization'], input_width=5, label_width=5, shift=1, label_columns=['x','y','yaw'])\n    example_window = tf.stack([np.array(train[key]['localization'][:w2.total_window_size]),\n                           np.array(train[key]['localization'][100:100+w2.total_window_size]),\n                           np.array(train[key]['localization'][200:200+w2.total_window_size])\n                           ])\n  \n    X , y = w2.split_window(example_window)\n    #print(X)\n    for X_i, y_i in zip(X , y):\n        #print(X_i.ndim, X_i.shape)\n        train_seq_X = np.append(train_seq_X,X_i) #,axis=0)\n        #train_seq_y = np.append(train_seq_y,y_i, axis=0)\n\ntrain_seq_X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:29:52.980796Z","iopub.execute_input":"2025-04-18T13:29:52.981184Z","iopub.status.idle":"2025-04-18T13:30:03.917406Z","shell.execute_reply.started":"2025-04-18T13:29:52.981150Z","shell.execute_reply":"2025-04-18T13:30:03.916550Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"array([ 0.00000000e+00, -3.51938598e+03, -1.39464479e+04, ...,\n        5.09623448e-03, -4.82397264e-03,  2.06305254e+00])"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"train_seq_X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:59:53.019771Z","iopub.execute_input":"2025-04-17T12:59:53.020188Z","iopub.status.idle":"2025-04-17T12:59:53.030781Z","shell.execute_reply.started":"2025-04-17T12:59:53.020155Z","shell.execute_reply":"2025-04-17T12:59:53.029486Z"}},"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"array([[[6.76819439e-310, 6.76819439e-310, 4.93452746e-310,\n         4.93452746e-310, 0.00000000e+000, 0.00000000e+000,\n         4.93451962e-310],\n        [4.93451962e-310, 5.43472210e-323, 0.00000000e+000,\n         3.95252517e-323, 8.48798316e-314, 3.95252517e-323,\n         0.00000000e+000],\n        [2.47032823e-323, 2.12199584e-314, 0.00000000e+000,\n         2.47032823e-323, 1.23516411e-322, 0.00000000e+000,\n         2.47032823e-323],\n        [4.24399163e-314, 0.00000000e+000, 2.47032823e-323,\n         4.24399163e-314, 0.00000000e+000, 2.47032823e-323,\n         6.36598742e-314],\n        [0.00000000e+000, 2.47032823e-323, 5.08887615e-322,\n         0.00000000e+000, 2.47032823e-323, 8.48798321e-314,\n         0.00000000e+000],\n        [2.47032823e-323, 2.12199587e-314, 0.00000000e+000,\n         2.47032823e-323, 1.06099790e-313, 0.00000000e+000,\n         2.47032823e-323],\n        [1.06099790e-313, 0.00000000e+000, 2.47032823e-323,\n         2.33419537e-313, 0.00000000e+000, 2.47032823e-323,\n         2.54639495e-313]],\n\n       [[0.00000000e+000, 2.96439388e-323, 2.75859454e-313,\n         0.00000000e+000, 2.96439388e-323, 4.99006302e-322,\n         0.00000000e+000],\n        [2.96439388e-323, 1.06099790e-313, 0.00000000e+000,\n         2.96439388e-323, 1.48539706e-313, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 1.69759663e-313, 1.35062725e+131,\n         1.23516411e-322, 0.00000000e+000, 2.96439388e-323,\n         2.12199584e-314],\n        [0.00000000e+000, 2.96439388e-323, 1.23516411e-322,\n         0.00000000e+000, 2.96439388e-323, 1.27319748e-313,\n         0.00000000e+000],\n        [2.96439388e-323, 2.33419538e-313, 0.00000000e+000,\n         2.96439388e-323, 2.97079411e-313, 0.00000000e+000,\n         2.96439388e-323],\n        [4.24399165e-314, 0.00000000e+000, 2.96439388e-323,\n         1.23516411e-322, 0.00000000e+000, 2.96439388e-323,\n         3.39519327e-313],\n        [1.69759663e-313, 3.95252517e-323, 4.99006302e-322,\n         0.00000000e+000, 3.45845952e-323, 1.06099790e-313,\n         0.00000000e+000]],\n\n       [[3.45845952e-323, 1.48539706e-313, 0.00000000e+000,\n         3.45845952e-323, 1.90979622e-313, 0.00000000e+000,\n         3.45845952e-323],\n        [1.23516411e-322, 0.00000000e+000, 3.45845952e-323,\n         2.12199584e-314, 0.00000000e+000, 3.45845952e-323,\n         1.23516411e-322],\n        [0.00000000e+000, 3.45845952e-323, 1.48539706e-313,\n         0.00000000e+000, 3.45845952e-323, 1.48539706e-313,\n         0.00000000e+000],\n        [3.45845952e-323, 2.33419538e-313, 0.00000000e+000,\n         3.45845952e-323, 2.97079411e-313, 0.00000000e+000,\n         3.45845952e-323],\n        [1.13635099e-322, 0.00000000e+000, 3.45845952e-323,\n         4.24399165e-314, 0.00000000e+000, 3.45845952e-323,\n         1.23516411e-322],\n        [0.00000000e+000, 3.45845952e-323, 2.12199587e-314,\n         0.00000000e+000, 3.45845952e-323, 4.99006302e-322,\n         0.00000000e+000],\n        [3.95252517e-323, 1.06099790e-313, 0.00000000e+000,\n         3.95252517e-323, 1.48539706e-313, 0.00000000e+000,\n         3.95252517e-323]],\n\n       [[1.90979622e-313, 0.00000000e+000, 3.95252517e-323,\n         1.23516411e-322, 0.00000000e+000, 3.95252517e-323,\n         2.12199584e-314],\n        [0.00000000e+000, 3.95252517e-323, 1.23516411e-322,\n         0.00000000e+000, 3.95252517e-323, 1.69759664e-313,\n         0.00000000e+000],\n        [3.95252517e-323, 1.69759664e-313, 0.00000000e+000,\n         3.95252517e-323, 2.33419538e-313, 0.00000000e+000,\n         3.95252517e-323],\n        [2.97079411e-313, 0.00000000e+000, 3.95252517e-323,\n         1.13635099e-322, 0.00000000e+000, 3.95252517e-323,\n         4.24399165e-314],\n        [0.00000000e+000, 3.95252517e-323, 1.23516411e-322,\n         0.00000000e+000, 3.95252517e-323, 2.12199587e-314,\n         0.00000000e+000],\n        [3.95252517e-323, 6.36598742e-314, 0.00000000e+000,\n         2.96439388e-323, 2.12199587e-314, 0.00000000e+000,\n         2.96439388e-323],\n        [3.18299369e-313, 6.57818695e-313, 7.42698527e-313,\n         2.33419537e-313, 0.00000000e+000, 5.43472210e-323,\n         3.39519327e-313]],\n\n       [[0.00000000e+000, 5.43472210e-323, 3.18299369e-313,\n         0.00000000e+000, 5.43472210e-323, 2.12199587e-314,\n         0.00000000e+000],\n        [5.43472210e-323, 4.24399163e-314, 0.00000000e+000,\n         5.43472210e-323, 3.60739285e-313, 0.00000000e+000,\n         5.43472210e-323],\n        [3.81959243e-313, 0.00000000e+000, 5.43472210e-323,\n         4.03179201e-313, 0.00000000e+000, 6.42285340e-323,\n         3.60739285e-313],\n        [0.00000000e+000, 6.42285340e-323, 3.81959243e-313,\n         0.00000000e+000, 6.42285340e-323, 4.24399165e-314,\n         0.00000000e+000],\n        [6.42285340e-323, 3.35964639e-322, 0.00000000e+000,\n         6.42285340e-323, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000]],\n\n       [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 6.76804946e-310, 8.48798316e-314,\n         3.95252517e-323]],\n\n       [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000],\n        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n         9.88131292e-324]]])"},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"len(train_seq_y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:44:26.540452Z","iopub.execute_input":"2025-04-15T13:44:26.540818Z","iopub.status.idle":"2025-04-15T13:44:26.546235Z","shell.execute_reply.started":"2025-04-15T13:44:26.540779Z","shell.execute_reply":"2025-04-15T13:44:26.545424Z"},"editable":false},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"122355"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_seq_X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:43:49.467471Z","iopub.execute_input":"2025-04-15T13:43:49.467923Z","iopub.status.idle":"2025-04-15T13:43:49.473995Z","shell.execute_reply.started":"2025-04-15T13:43:49.467890Z","shell.execute_reply":"2025-04-15T13:43:49.472953Z"},"editable":false},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"array([ 0.00000000e+00, -3.51938598e+03, -1.39464479e+04, ...,\n        3.81686862e-02, -2.76660030e-02, -7.57760663e-01])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"w2 = WindowGenerator(train=train, val=test, label_columns=['x','y','yaw'], input_width=5, label_width=5, shift=1)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w2","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w2 = WindowGenerator(train_df=train[3]['localization'], val_df=test[2]['localization'], label_columns=['x','y','yaw'], input_width=5, label_width=5, shift=1)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_window = tf.stack([np.array(train[3]['localization'][:w2.total_window_size]),\n                           np.array(train[3]['localization'][100:100+w2.total_window_size]),\n                           np.array(train[3]['localization'][200:200+w2.total_window_size])\n                           ])\n\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_inputs, example_labels = w2.split_window(example_window)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'Labels shape: {example_labels.shape}')\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_dataset(self, data):\n    data = np.array(el, dtype=np.float32)\n    ds = tf.keras.utils.timeseries_dataset_from_array(\n        data=data,\n        targets=None,\n        sequence_length=self.total_window_size,\n        sequence_stride=1,\n        shuffle=True,\n        batch_size=32,)\n\n    ds = ds.map(self.split_window)\n\n    return ds\n\nWindowGenerator.make_dataset = make_dataset","metadata":{"trusted":true,"_kg_hide-input":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@property\ndef train(self):\n    for i in train_ids:\n        train_df = train[i]['localization']\n        return self.make_dataset(self.train_df)\n\n\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!install paramiko","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wide_window = WindowGenerator( train_df=train[3]['localization'], val_df=test[2]['localization'], label_columns=['x','y','yaw'],\n    input_width=24, label_width=24, shift=1,)\n\nwide_window","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_EPOCHS = 20\n\ndef compile_and_fit(model, window, patience=2):\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(),\n                metrics=[tf.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=1)\n])","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = compile_and_fit(lstm_model, w2)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[5]['control']['stamp_ns'] = train_dataset[5]['control']['stamp_ns']/10e9\ntrain_dataset[5]['localization']['stamp_ns'] = train_dataset[5]['localization']['stamp_ns']/10e9","metadata":{"id":"OeZZt97HhVXx","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.DataFrame()\nfor el in train_dataset:\n    ","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[25]['control']","metadata":{"id":"KA260PoHhVXx","outputId":"da992b0b-83e6-4f25-8133-27a78752809e","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[25]['control'].drop(columns='stamp_ns')","metadata":{"id":"NWQ_2soIhVXy","outputId":"a05bb37d-f45d-4a27-def6-ea9df8d34fc2","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.concat([df1.set_index('A'),df2.set_index('A')], axis=1, join='inner').reset_index()\n","metadata":{"id":"bYBB0IozhVXy","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = train_dataset[25]['localization'][74:1574]","metadata":{"id":"62-KhtXUhVXy","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[25]['control'].drop(columns='stamp_ns')","metadata":{"id":"hSK_thHihVXy","outputId":"cf50cab2-49f8-4e1a-df75-7b5f71862f3a","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"DfE1m6kEhVXz","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.concat([train_dataset[25]['localization'][74:1574].reset_index(),train_dataset[25]['control'].drop(columns='stamp_ns').reset_index()], axis=1).reset_index().drop(columns=['index','level_0'])","metadata":{"id":"XvyaFXC9hVXz","outputId":"2787c9c0-3b78-4397-c79a-fed5a2d7bce8","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = read_testcases(TEST_DATASET_PATH, is_test=True)\nlen(test_dataset)","metadata":{"id":"ClsonwZGhVXz","outputId":"9ffc45a7-bbc5-419f-ef20-bafc1083766b","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loc stams x y z roll pitch yaw 0--> LSTM --> next 1 loc stams x y z roll pitch yaw","metadata":{"id":"vONDOzGAhVXz","outputId":"7411393a-957f-45be-9477-02aa205d9640","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[el.values() for el in test_dataset.values()]","metadata":{"id":"nG782-XLhVXz","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"65HgYCoYhVX0","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LSTM conv 1d","metadata":{"id":"K8wl-k42hVX0","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.layers.recurrent import LSTM","metadata":{"id":"aW03q1p8hVX0","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.python.keras.layers.recurrent import LSTM, Bidirectional","metadata":{"id":"YbaN3leohVX0","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"aq3X2UJthVX0","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential([\n    LSTM(50, activation='tanh', return_sequences=True, input_shape=(10, 5)),  # First LSTM layer\n    LSTM(30, activation='tanh'),  # Second LSTM layer\n    Dense(1, activation='sigmoid')  # Output layer for binary classification\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"id":"KiaEWVBbhVX0","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.keras.layers.Bidirectional(\n    layer, merge_mode='concat'\n)\n","metadata":{"id":"v_agCQ89hVX1","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dataprepare():\n    def __init__(dataset):\n        self.dataset = dataset\n\n\n    def __len__(self):\n        return len(self.target)\n\n    def __getitem__(self, idx):\n        current_target = self.target[idx]\n        current_tweet = self.tweets[idx]\n        sequence = []\n        for word in current_tweet:\n            if word in self.word_to_idx.keys():\n                sequence.append(self.word_to_idx[word])\n\n        return {\n                'x': torch.tensor(sequence, dtype=torch.long),\n                'y': torch.tensor(current_target, dtype=torch.long)\n            }\n","metadata":{"id":"I8Uu5ILEhVX1","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"DeTWySF8hVX1","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"zvI7t1u1hVX1","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(data)","metadata":{"id":"iWU3bmJLhVX1","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataset(data, time_step=60):\n    X, y = [], []\n    for i in range(len(data) - time_step - 1):\n        X.append(data[i:(i + time_step), 0])\n        y.append(data[i + time_step, 0])\n    return np.array(X), np.array(y)\n\nX, y = create_dataset(scaled_data)\nX = X.reshape(X.shape[0], X.shape[1], 1)","metadata":{"id":"VRpukKQBhVX1","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size = int(len(X) * 0.8)\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]","metadata":{"id":"RMl7fjevhVX2","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size = int(len(X) * 0.8)\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]","metadata":{"id":"0aU-KS-ZhVX2","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"cDyOoP6WhVX2","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dummy baseline","metadata":{"id":"6mgJHUS1hVX2","editable":false}},{"cell_type":"markdown","source":"### read test dataset","metadata":{"id":"3d0nSUy5hVX2","editable":false}},{"cell_type":"code","source":"import numpy as np\n\nNSECS_IN_SEC = 1000000000\n\ndef secs_to_nsecs(secs: float):\n    return int(secs * NSECS_IN_SEC)\n\ndef nsecs_to_secs(nsecs: int):\n    return float(nsecs) / NSECS_IN_SEC\n\ndef yaw_direction(yaw_value):\n    return np.array([np.cos(yaw_value), np.sin(yaw_value)])","metadata":{"id":"2NgfRhWLhVX2","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install keras==2.10.0","metadata":{"id":"aF1vo_RhhVX9","outputId":"2ded429f-567a-4db7-eda6-10de07339d04","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"auiYitr-hVX9","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nnp.__version__","metadata":{"id":"XYF5YQ6RhVX9","outputId":"c4c94bb6-d1b8-4e42-edad-6d1f89f95bd4","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### simple pose prediction logic without taking into account control states","metadata":{"id":"kfbYDFqLhVX-","editable":false}},{"cell_type":"code","source":"","metadata":{"id":"CMKRwdMGhVX-","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def localization_df_to_poses(loc, contrl):\n    loc_contrl = pd.concat([loc[74:1574].reset_index(),contrl.drop(columns='stamp_ns').reset_index()], axis=1).reset_index().drop(columns=['index','level_0'])\n    poses = []\n    for stamp_ns, x, y, yaw,acceleration_level, steering, in zip(loc_contrl['stamp_ns'], loc_contrl['x'], loc_contrl['y'], loc_contrl['yaw'], loc_contrl['acceleration_level'],loc_contrl['steering']):\n        poses.append({'stamp_ns': stamp_ns, 'pos': np.array([x, y]), 'yaw': yaw})\n    return poses\n","metadata":{"id":"o2g_PX8FhVX-","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def localization_df_to_poses(loc_df):\n    poses = []\n    for stamp_ns, x, y, yaw in zip(loc_df['stamp_ns'], loc_df['x'], loc_df['y'], loc_df['yaw']):\n        poses.append({'stamp_ns': stamp_ns, 'pos': np.array([x, y]), 'yaw': yaw})\n    return poses","metadata":{"id":"Ex6R_lNAhVX-","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"localization_df_to_poses(loc_df)\n","metadata":{"id":"0R6NGBGIhVX-","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport numpy as np","metadata":{"id":"tOJy-3a7hVX-","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def localization_df_to_poses(loc_df):\n    poses = []\n    for stamp_ns, x, y, yaw in zip(loc_df['stamp_ns'], loc_df['x'], loc_df['y'], loc_df['yaw']):\n        poses.append({'stamp_ns': stamp_ns, 'pos': np.array([x, y]), 'yaw': yaw})\n    return poses\n\n# naive estimation of speed at last known localization pose\ndef dummy_estimate_last_speed(localization_poses):\n    last_pose = localization_poses[-1]\n\n    start_pose_idx = -1\n    for i, pose in enumerate(localization_poses, start=1-len(localization_poses)):\n        start_pose_idx = i\n        if nsecs_to_secs(last_pose['stamp_ns']) - nsecs_to_secs(pose['stamp_ns']) > 1.: # sec\n            break\n\n    start_pose = localization_poses[start_pose_idx]\n    dt_sec = nsecs_to_secs(last_pose['stamp_ns']) - nsecs_to_secs(start_pose['stamp_ns'])\n\n    if dt_sec > 1e-5:\n        return np.linalg.norm(last_pose['pos'][:2] - start_pose['pos'][:2]) / dt_sec\n    return 5. # some default value\n\ndef dummpy_predict_pose(last_loc_pose: dict, last_speed: float, prediction_stamp: int):\n    dt_sec = nsecs_to_secs(prediction_stamp) - nsecs_to_secs(last_loc_pose['stamp_ns'])\n    distance = dt_sec * last_speed\n    direction = yaw_direction(last_loc_pose['yaw'])\n    pos_translate = direction * distance\n    return {\"pos\": last_loc_pose['pos'] + pos_translate, 'yaw': last_loc_pose['yaw']}","metadata":{"id":"a7DB9tpLhVX_","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_testcase(testcase: dict):\n    loc_df = testcase['localization']\n    localization_poses = localization_df_to_poses(loc_df)\n\n    last_loc_pose = localization_poses[-1]\n    last_speed = dummy_estimate_last_speed(localization_poses)\n\n    predicted_poses = []\n    for stamp in testcase['requested_stamps']['stamp_ns']:\n        pose = dummpy_predict_pose(last_loc_pose, last_speed, stamp)\n        predicted_poses.append(pose)\n\n    predictions = {}\n    predictions['stamp_ns'] = testcase['requested_stamps']['stamp_ns']\n    predictions['x'] = [pose['pos'][0] for pose in predicted_poses]\n    predictions['y'] = [pose['pos'][1] for pose in predicted_poses]\n    predictions['yaw'] = [pose['yaw'] for pose in predicted_poses]\n    return pd.DataFrame(predictions)\n\ndef predict_test_dataset(test_dataset: dict):\n    predictions = {}\n    for testcase_id, testcase in tqdm(test_dataset.items()):\n        predictions[testcase_id] = predict_testcase(testcase)\n    return predictions","metadata":{"id":"z3wAYeHghVX_","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### make prediction for requested stamps","metadata":{"id":"0bvydxAChVX_","editable":false}},{"cell_type":"code","source":"predictions = predict_test_dataset(train_dataset)\nlen(predictions)","metadata":{"id":"U9i8cnA_hVX_","outputId":"71cce3bb-ac19-4a5f-9957-ce0a1b4700bb","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predictions = predict_test_dataset(test_dataset)\nlen(test_predictions)","metadata":{"scrolled":true,"id":"uPoDBBjchVYA","outputId":"98538af8-eec6-43eb-f8e7-879d657db88d","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predictions","metadata":{"id":"u3oIJMXUhVYA","outputId":"588fcff9-0929-42b3-8d09-cba8ad5bcfd6","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### write predictions","metadata":{"id":"LziiM8AuhVYA","editable":false}},{"cell_type":"code","source":"def write_predictions(dataset_predictions: dict, prediction_file_path: str):\n    prediction_list = []\n    for testcase_id, prediction in tqdm(dataset_predictions.items()):\n        prediction['testcase_id'] = [testcase_id] * len(prediction)\n        prediction_list.append(prediction)\n    predictions_df = pd.concat(prediction_list)\n    predictions_df = predictions_df.reindex(columns=[\"testcase_id\", \"stamp_ns\", \"x\", \"y\", \"yaw\"])\n    print(len(predictions_df))\n    predictions_df.to_csv(prediction_file_path, index=False, header=True)","metadata":{"id":"EmtU7NwghVYA","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"write_predictions(test_predictions, os.path.join(ROOT_DATA_FOLDER, \"predictions.csv\"))","metadata":{"id":"e_RGLsQLhVYB","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Calculate metric","metadata":{"id":"gdcQt2j1hVYB","editable":false}},{"cell_type":"markdown","source":"Let's describe final metric. As a first step, all predicted triples $(x,y,yaw)$ are being converted into 2 points $[(x_1, y_1), (x_2, y_2)]$ in the following way:\n$$\n(x_1, y_1) = (x, y), \\\\\n(x_2, y_2) = (x_1, y_1) + S \\times (yaw_x, yaw_y)\n$$  \n\nwhere $S = 1$. In other words, we build a directed segment of length $1$. These points then used in the metric calculation.\n\n\nMetric for a single pose (rmse):\n\n$$\npose\\_metric = \\sqrt{ \\frac{\\displaystyle\\sum_{j=1}^{k} {(x_j-\\hat{x_j})^2 + (y_j-\\hat{y_j})^2}}{k} }\n$$\n\nwhere $k$ - number of points that describe single pose (in our case $k=2$).\n\nMetric for a testcase:\n\n$$\ntestcase\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}pose\\_metric_i\n$$\n\nwhere $n$ - number of localization points to predict.\n\nAnd, final metric for a whole dataset:\n\n$$\ndataset\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}testcase\\_metric_i\n$$\n\nwhere $n$ - number of test cases.\n","metadata":{"id":"9y9Xz0j3hVYB","editable":false}},{"cell_type":"markdown","source":"### implementation of the metric calculation","metadata":{"id":"XtL6RI_MhVYB","editable":false}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nSEGMENT_LENGTH = 1.\n\ndef yaw_direction(yaw_value):\n    return np.array([np.cos(yaw_value), np.sin(yaw_value)])\n\ndef build_car_points(x_y_yaw):\n    directions = np.vstack(yaw_direction(x_y_yaw[:, -1]))\n\n    front_points = x_y_yaw[:, :-1] + SEGMENT_LENGTH * directions.T\n    points = np.vstack([x_y_yaw[:, :-1], front_points])\n    return points\n\ndef build_car_points_from_merged_df(df: pd.DataFrame):\n    points_gt = df[['x_gt', 'y_gt', 'yaw_gt']].to_numpy()\n    points_pred = df[['x_pred', 'y_pred', 'yaw_pred']].to_numpy()\n\n    points_gt = build_car_points(points_gt)\n    points_pred = build_car_points(points_pred)\n    return points_gt, points_pred\n\ndef calculate_metric_testcase(df: pd.DataFrame):\n    points_gt, points_pred = build_car_points_from_merged_df(df)\n\n    metric = np.mean(np.sqrt(2. * np.mean((points_gt - points_pred) ** 2, axis=1)))\n    return metric\n\ndef calculate_metric_dataset(ground_truth_df: pd.DataFrame, prediction_df: pd.DataFrame):\n    assert (len(ground_truth_df) == len(prediction_df))\n\n    df = ground_truth_df.merge(prediction_df, on=['testcase_id', 'stamp_ns'], suffixes=['_gt', '_pred'])\n\n    metric = df.groupby('testcase_id').apply(calculate_metric_testcase)\n    return np.mean(metric)","metadata":{"id":"G8A_yn3AhVYC","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"jwdtOh5-hVYC","trusted":true,"editable":false},"outputs":[],"execution_count":null}]}